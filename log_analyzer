#!/usr/bin/env python

import sys
from pyspark import SparkContext, SparkConf

conf = SparkConf().setAppName("log_analyzer").setMaster("local")
sc = SparkContext(conf=conf)

input_text_file=sys.argv[1]
input_text_file_2=sys.argv[2]
#chmod +x log_analyzer
#./log_analyzer /home/jenkin/hadoop/spark/Assignment-2/Log/iliad /home/jenkin/hadoop/spark/Assignment-2/Log/odyssey
#./log_analyzer hdfs://localhost:9000/user/jenkin/Log/iliad hdfs://localhost:9000/user/jenkin/Log/odyssey

#hdfs://localhost:9000/Log/iliad
#hdfs://localhost:9000/Log/odyssey

#./log_analyzer Log/iliad Log/odyssey

lines = sc.textFile(input_text_file)
cc = lines.count()
print(cc)
c = lines.filter(lambda x:x.find("Starting")!=-1 and x.find("Session")!=-1 and x.find("achille")!=-1).count()
print(c)

lines = sc.textFile(input_text_file_2)
cc = lines.count()
print(cc)
c = lines.filter(lambda x:x.find("Starting")!=-1 and x.find("Session")!=-1 and x.find("achille")!=-1).count()
print(c)
